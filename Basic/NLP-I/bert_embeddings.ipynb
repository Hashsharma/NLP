{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d320772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Embeddings + Visualization\n",
    "# The vector for a word depends on the sentence it appears in.\n",
    "# ‚Äúbank of the river‚Äù ‚Üí bank = river side\n",
    "# ‚Äúbank approved the loan‚Äù ‚Üí bank = financial institution\n",
    "# üëâ Word2Vec gives one vector per word\n",
    "# üëâ BERT gives one vector per word per context\n",
    "# | Feature            | Word2Vec / FastText | BERT            |\n",
    "# | ------------------ | ------------------- | --------------- |\n",
    "# | Embedding type     | Static              | Contextual      |\n",
    "# | Polysemy handling  | ‚ùå No               | ‚úÖ Yes          |\n",
    "# | Architecture       | Shallow NN          | Transformer     |\n",
    "# | Training objective | Context prediction  | Masked LM + NSP |\n",
    "# | Dimensionality     | 50‚Äì300              | 768 (base)      |\n",
    "# | Used in modern NLP | ‚ùå Rare             | ‚úÖ Standard     |\n",
    "\n",
    "# BERT embeddings capture syntax + semantics + context.\n",
    "# 2. Next-Gen Architectures Going Beyond Classic Transformers\n",
    "# üîπ Titans (Google‚Äôs Proposed Successor to Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa1b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb31c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/media/scientist-anand/volume/mr_document/all_models/bert-based-uncased\")\n",
    "model = BertModel.from_pretrained(\"/media/scientist-anand/volume/mr_document/all_models/bert-based-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174dbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This product is amazing\",\n",
    "    \"I hate this service\",\n",
    "    \"This is the worst experience\"\n",
    "]\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505bda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence 1: [101, 7592, 2088, 102]\n",
    "# Sentence 2: [101, 2023, 2003, 1037, 7953, 102]\n",
    "\n",
    "# After padding:\n",
    "# [\n",
    "#  [101, 7592, 2088, 102,   0,   0],\n",
    "#  [101, 2023, 2003, 1037, 7953, 102]\n",
    "# ]\n",
    "# 1 = real token\n",
    "# 0 = padding token\n",
    "\n",
    "# Tells PyTorch:\n",
    "\n",
    "# torch.no_grad() ‚ÄúI‚Äôm not training, just doing inference‚Äù\n",
    "# Effects\n",
    "# No gradients are stored\n",
    "# No computation graph is built\n",
    "# Faster execution\n",
    "# Much lower memory usage\n",
    "# When to use\n",
    "\n",
    "# Evaluation\n",
    "# Embedding extraction\n",
    "# Inference / prediction\n",
    "\n",
    "# Keeps only the first 512 tokens\n",
    "# Deletes the rest permanently\n",
    "# If every detail matters, then simple truncation is NOT acceptable. You must process long text in pieces and then combine the information.\n",
    "# Yes ‚Äî this is a neural-network architecture limitation, not an arbitrary tokenizer rule.\n",
    "# O(n¬≤)\n",
    "# 512¬≤ = 262,144 attention interactions\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True, # Max Length 512 if it is more than that it will remove or truncate to 512 if its 600\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad(): # ‚ÄúI‚Äôm not training, just doing inference‚Äù\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "token_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d48b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Model       | Trick                             |\n",
    "# | ----------- | --------------------------------- |\n",
    "# | Longformer  | Sparse attention                  |\n",
    "# | BigBird     | Block + random attention          |\n",
    "# | RoPE models | Mathematical position encoding    |\n",
    "# | ALiBi       | Linear bias instead of embeddings |\n",
    "\n",
    "# [CLS] = Classification token\n",
    "# It is a special token added at the beginning of the input.\n",
    "\n",
    "# [CLS] is used for Next Sentence Prediction (NSP)\n",
    "# [SEP] = Separator token\n",
    "\n",
    "# End of a sentence\n",
    "# Boundary between two sentences\n",
    "\n",
    "# [CLS] This is a sentence [SEP]\n",
    "# I love machine learning. It is very powerful.\n",
    "# [CLS] I love machine learning . [SEP] It is very powerful . [SEP]\n",
    "# ========================================================================= #\n",
    "# (batch_size, seq_len, hidden_size)\n",
    "# | Dimension   | Meaning                                                       |\n",
    "# | ----------- | ------------------------------------------------------------- |\n",
    "# | batch_size  | Number of sentences or sequences in the batch (`n_sentences`) |\n",
    "# | seq_len     | Number of tokens in each sentence (after padding/truncation)  |\n",
    "# | hidden_size | Size of the model‚Äôs hidden layer (BERT-base: 768)             |\n",
    "\n",
    "# Why [:, 0, :]?\n",
    "\n",
    "# : ‚Üí select all sentences in the batch\n",
    "# 0 ‚Üí select token at position 0 ‚Üí this is [CLS]\n",
    "# : ‚Üí select all hidden dimensions (full embedding vector)\n",
    "\n",
    "# token_embeddings (batch_size, seq_len, hidden_size)\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë [CLS] token ‚ïë ‚Üê position 0 ‚Üí vector for entire sentence\n",
    "# ‚ïë token 1     ‚ïë\n",
    "# ‚ïë token 2     ‚ïë\n",
    "# ‚ïë ...         ‚ïë\n",
    "# ‚ïë [SEP] token ‚ïë ‚Üê last token\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "# \"with\" A context manager handles setup and cleanup automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccfeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs[\"attention_mask\"] is automatically generated by the tokenizer.\n",
    "# It‚Äôs a tensor of 1s and 0s indicating real tokens vs padding.\n",
    "# It is passed to the model alongside input_ids for correct attention computation.\n",
    "    \n",
    "token_embeddings.shape  # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ec7f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_mask = tensor([\n",
    "#   [1, 1, 1, 1, 0, 0],\n",
    "#   [1, 1, 1, 1, 1, 0]\n",
    "# ])\n",
    "# unsqueeze(-1)\n",
    "# attention_mask.unsqueeze(-1) adds a new dimension at the last position:\n",
    "# attention_mask.unsqueeze(-1).shape  # (batch_size, seq_len, 1)\n",
    "# Why? Because we want to match the shape of token_embeddings along the embedding dimension so we can multiply or mask them elementwise.\n",
    "# [[[1], [1], [1], [1], [0], [0]],\n",
    "#  [[1], [1], [1], [1], [1], [0]]]\n",
    "\n",
    "# expand(token_embeddings.size())\n",
    "# .expand() repeats the tensor along the new dimensions without copying memory, making it the same shape as token_embeddings:\n",
    "# mask.shape  # (batch_size, seq_len, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f149d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean pooling (better in practice) \n",
    "# Mean pooling often performs better than raw [CLS].\n",
    "\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e09a4174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7])\n",
      "torch.Size([4, 7, 768])\n",
      "torch.Size([4, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask.shape)\n",
    "mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()) ## Adding to make it the same size as token embeddings -1 means at end\n",
    "print(mask.shape)\n",
    "masked_embeddings = token_embeddings * mask\n",
    "print(masked_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f50b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatterbox_venv",
   "language": "python",
   "name": "chatterbox_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
