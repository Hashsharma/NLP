{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anand\n",
      "[nltk_data]     Vishwakarma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Anand\n",
      "[nltk_data]     Vishwakarma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anand\n",
      "[nltk_data]     Vishwakarma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Anand\n",
      "[nltk_data]     Vishwakarma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download necessary data files (if not done already)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_tokenize',\n",
       " 'splits',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " 'and',\n",
       " 'punctuation',\n",
       " 'as',\n",
       " 'separate',\n",
       " 'tokens',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'Punkt',\n",
       " 'tokenizer',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pre-trained',\n",
       " 'model',\n",
       " 'for',\n",
       " 'tokenizing',\n",
       " 'text',\n",
       " 'in',\n",
       " 'many',\n",
       " 'languages',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"word_tokenize splits the text into words and punctuation as separate tokens It uses the Punkt tokenizer, which is a pre-trained model for tokenizing text in many languages.\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_tokenize',\n",
       " 'splits',\n",
       " 'text',\n",
       " 'words',\n",
       " 'punctuation',\n",
       " 'separate',\n",
       " 'tokens',\n",
       " 'it',\n",
       " 'uses',\n",
       " 'punkt',\n",
       " 'tokenizer',\n",
       " ',',\n",
       " 'pre-trained',\n",
       " 'model',\n",
       " 'tokenizing',\n",
       " 'text',\n",
       " 'many',\n",
       " 'languages',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_words = [word.lower() for word in tokens if word not in stop_words]\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_token',\n",
       " 'split',\n",
       " 'text',\n",
       " 'word',\n",
       " 'punctuat',\n",
       " 'separ',\n",
       " 'token',\n",
       " 'it',\n",
       " 'use',\n",
       " 'punkt',\n",
       " 'token',\n",
       " ',',\n",
       " 'pre-train',\n",
       " 'model',\n",
       " 'token',\n",
       " 'text',\n",
       " 'mani',\n",
       " 'languag',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Porter stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "stem = [stemming.stem(w) for w in clean_words]\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_token',\n",
       " 'split',\n",
       " 'text',\n",
       " 'word',\n",
       " 'punctuat',\n",
       " 'separ',\n",
       " 'token',\n",
       " 'it',\n",
       " 'use',\n",
       " 'punkt',\n",
       " 'token',\n",
       " ',',\n",
       " 'pre-train',\n",
       " 'model',\n",
       " 'token',\n",
       " 'text',\n",
       " 'mani',\n",
       " 'languag',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Porter SnoballStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemming = SnowballStemmer(\"english\")\n",
    "stem = [stemming.stem(w) for w in clean_words]\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anand\n",
      "[nltk_data]     Vishwakarma\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_tokenize',\n",
       " 'split',\n",
       " 'text',\n",
       " 'word',\n",
       " 'punctuation',\n",
       " 'separate',\n",
       " 'token',\n",
       " 'it',\n",
       " 'us',\n",
       " 'punkt',\n",
       " 'tokenizer',\n",
       " ',',\n",
       " 'pre-trained',\n",
       " 'model',\n",
       " 'tokenizing',\n",
       " 'text',\n",
       " 'many',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word_lemmatizer = [lemmatizer.lemmatize(word) for word in clean_words]\n",
    "word_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_tokenize',\n",
       " 'split',\n",
       " 'text',\n",
       " 'word',\n",
       " 'punctuation',\n",
       " 'separate',\n",
       " 'token',\n",
       " 'it',\n",
       " 'use',\n",
       " 'punkt',\n",
       " 'tokenizer',\n",
       " ',',\n",
       " 'pre-trained',\n",
       " 'model',\n",
       " 'tokenizing',\n",
       " 'text',\n",
       " 'many',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Part Of Speech and lemmatizer\n",
    "pos = nltk.pos_tag(clean_words)\n",
    "pos_lemm = [lemmatizer.lemmatize(word, pos='v' if tag.startswith('V') else 'n') for word, tag in pos]\n",
    "pos_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Apple Inc. is planning to open a new store in New York City on March 10, 2025.,\n",
       " (Apple Inc., New York City, March 10, 2025))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text2 = \"Apple Inc. is planning to open a new store in New York City on March 10, 2025.\"\n",
    "\n",
    "## Named Entity Recognition (NER)\n",
    "ner_doc = nlp(text2)\n",
    "ner_doc, ner_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Inc.\n",
      "is\n",
      "planning\n",
      "to\n",
      "open\n",
      "a\n",
      "new\n",
      "store\n",
      "in\n",
      "New\n",
      "York\n",
      "City\n",
      "on\n",
      "March\n",
      "10\n",
      ",\n",
      "2025\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for ent in ner_doc:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, Phrases, and Concepts:\n",
      "Punkt (ORG)\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Entity Labels:\n",
    "# Here are some common entity labels you'll encounter:\n",
    "\n",
    "# PERSON: A personâ€™s name (e.g., \"Barack Obama\").\n",
    "# ORG: Organization (e.g., \"Apple\", \"United Nations\").\n",
    "# GPE: Geopolitical entity, typically a country or city (e.g., \"New York\", \"Germany\").\n",
    "# DATE: Date or time expression (e.g., \"March 10, 2025\").\n",
    "# LOC: Location (e.g., \"Mount Everest\").\n",
    "# MONEY: Monetary values (e.g., \"$100\", \"â‚¬50\").\n",
    "# TIME: Time expressions (e.g., \"2 PM\", \"midnight\").\n",
    "# Extract named entities\n",
    "print(\"Named Entities, Phrases, and Concepts:\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_venv",
   "language": "python",
   "name": "nltk_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
