{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anand\n",
      "[nltk_data]     Vishwakarma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['artificial',\n",
       "  'intelligence',\n",
       "  'is',\n",
       "  'transforming',\n",
       "  'various',\n",
       "  'industries',\n",
       "  ',',\n",
       "  'from',\n",
       "  'healthcare',\n",
       "  'to',\n",
       "  'finance.',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'algorithms',\n",
       "  'are',\n",
       "  'being',\n",
       "  'applied',\n",
       "  'to',\n",
       "  'analyze',\n",
       "  'large',\n",
       "  'datasets',\n",
       "  ',',\n",
       "  'uncover',\n",
       "  'patterns',\n",
       "  ',',\n",
       "  'and',\n",
       "  'make',\n",
       "  'predictions',\n",
       "  '.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Gensim\n",
    "\n",
    "text = '''Artificial intelligence is transforming various industries, from healthcare to finance. Machine learning algorithms are being applied to analyze large datasets, uncover patterns, and make predictions.'''\n",
    "\n",
    "\n",
    "## Tokenizing the models\n",
    "\n",
    "tokenizer = NLTKWordTokenizer()\n",
    "token = tokenizer.tokenize(text.lower())\n",
    "# # Tokenize the text\n",
    "# tokens = word_tokenize(text.lower())  # Lowercasing for uniformity\n",
    "\n",
    "# Prepare the model with tokenized sentences\n",
    "sentences = [token]\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('applied', 0.27649635076522827), ('to', 0.21919605135917664), ('is', 0.2144073098897934), ('from', 0.2006339430809021), ('uncover', 0.156716987490654), ('algorithms', 0.11888141185045242), ('various', 0.0926261767745018), ('learning', 0.0898841992020607), ('machine', 0.07573571801185608), (',', 0.07528750598430634)]\n"
     ]
    }
   ],
   "source": [
    "## Passing to the Word2Vec model\n",
    "\n",
    "# Example: Get the vector for a word\n",
    "# vector = model.wv['artificial']\n",
    "\n",
    "# # Example: Find the most similar words to \"artificial\"\n",
    "# similar_words = model.wv.most_similar('artificial')\n",
    "\n",
    "# vector_size=50:\n",
    "\n",
    "# This specifies the size of the embedding vector for each word. In this case, each word will be represented as a 50-dimensional vector in the vector space.\n",
    "# Larger vectors may capture more semantic information, but they require more computational resources. 50 dimensions are a good starting point for most cases.\n",
    "# window=5:\n",
    "\n",
    "# This defines the context window size, which controls how many words before and after a target word are considered as context during training.\n",
    "# For example, if the window size is 5, the model will look at the 5 words before and 5 words after the target word to learn the context. A larger window can capture broader relationships but may introduce noise.\n",
    "# min_count=1:\n",
    "\n",
    "# This sets the minimum number of occurrences a word must have to be considered during training.\n",
    "# If a word appears fewer than min_count times in the dataset, it will be ignored. In this case, setting min_count=1 means that even words that appear once in the dataset will be considered in training.\n",
    "# A higher min_count (e.g., 5 or 10) is usually used to filter out rare words or noise.\n",
    "# workers=4:\n",
    "\n",
    "# This defines how many CPU cores to use during training to speed up the process. Gensim will train the model in parallel using 4 CPU cores.\n",
    "# More workers can speed up the training time, especially on large datasets, but make sure your machine has enough resources.\n",
    "\n",
    "wv_embedding = Word2Vec(sentences, window=5, vector_size=50, min_count=1, workers=4)\n",
    "\n",
    "## Vector file\n",
    "vector = wv_embedding.wv['artificial']\n",
    "\n",
    "## Similar words\n",
    "similar_words = wv_embedding.wv.most_similar('artificial')\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01333999, -0.01366328,  0.00191952, -0.01201695,  0.00329469,\n",
       "       -0.00857856, -0.00688159,  0.00437133,  0.01732315,  0.01345622,\n",
       "       -0.01935411, -0.01124421,  0.01576067,  0.00397871, -0.0085121 ,\n",
       "        0.00119762,  0.01904192, -0.00220543, -0.01884928,  0.00321682,\n",
       "        0.01246471,  0.01256474,  0.0081833 , -0.01130048, -0.00074139,\n",
       "       -0.00011064,  0.00914359, -0.01608318, -0.01603662,  0.0005295 ,\n",
       "       -0.0172166 ,  0.01164031, -0.00083562,  0.01994235, -0.01068795,\n",
       "       -0.00097228,  0.01551355, -0.00813586, -0.0100318 ,  0.00318014,\n",
       "        0.00530139, -0.00512992,  0.01289506, -0.01531991,  0.00678712,\n",
       "        0.00097994,  0.01746437,  0.01196543,  0.01363072,  0.01564509],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_venv",
   "language": "python",
   "name": "nltk_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
