{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mr_document\\all_venv\\chat_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import torch\n",
    "from transformers import AutoModelForCausalLM, GemmaTokenizer\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "from gtts import gTTS\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma3ForCausalLM were not initialized from the model checkpoint at D:/mr_document/all_models/gemma3/ and are newly initialized: ['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.post_feedforward_layernorm.weight', 'model.layers.0.pre_feedforward_layernorm.weight', 'model.layers.0.self_attn.k_norm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_norm.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.post_feedforward_layernorm.weight', 'model.layers.1.pre_feedforward_layernorm.weight', 'model.layers.1.self_attn.k_norm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_norm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.post_feedforward_layernorm.weight', 'model.layers.10.pre_feedforward_layernorm.weight', 'model.layers.10.self_attn.k_norm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_norm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.post_feedforward_layernorm.weight', 'model.layers.11.pre_feedforward_layernorm.weight', 'model.layers.11.self_attn.k_norm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_norm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.post_feedforward_layernorm.weight', 'model.layers.12.pre_feedforward_layernorm.weight', 'model.layers.12.self_attn.k_norm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_norm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.post_feedforward_layernorm.weight', 'model.layers.13.pre_feedforward_layernorm.weight', 'model.layers.13.self_attn.k_norm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_norm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_norm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_norm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.post_feedforward_layernorm.weight', 'model.layers.2.pre_feedforward_layernorm.weight', 'model.layers.2.self_attn.k_norm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_norm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.post_feedforward_layernorm.weight', 'model.layers.3.pre_feedforward_layernorm.weight', 'model.layers.3.self_attn.k_norm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_norm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.post_feedforward_layernorm.weight', 'model.layers.4.pre_feedforward_layernorm.weight', 'model.layers.4.self_attn.k_norm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_norm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.post_feedforward_layernorm.weight', 'model.layers.5.pre_feedforward_layernorm.weight', 'model.layers.5.self_attn.k_norm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_norm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.post_feedforward_layernorm.weight', 'model.layers.6.pre_feedforward_layernorm.weight', 'model.layers.6.self_attn.k_norm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_norm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.post_feedforward_layernorm.weight', 'model.layers.7.pre_feedforward_layernorm.weight', 'model.layers.7.self_attn.k_norm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_norm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.post_feedforward_layernorm.weight', 'model.layers.8.pre_feedforward_layernorm.weight', 'model.layers.8.self_attn.k_norm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_norm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.post_feedforward_layernorm.weight', 'model.layers.9.pre_feedforward_layernorm.weight', 'model.layers.9.self_attn.k_norm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_norm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path=\"D:/mr_document/all_models/gemma3-it/\"\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs=['What is your favorite food!']\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def tokenize_data(inputs, tokenizer, max_length=64):\n",
    "    input_encodings = tokenizer(\n",
    "        list(inputs), max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return input_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "        }\n",
    "\n",
    "test_inputs_enc = tokenize_data(test_inputs, tokenizer)\n",
    "test_dataset = CustomDataset(test_inputs_enc)  # no labels\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    input_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        repetition_penalty=1.2,  \n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your favorite food!Managed ‡∞ú‡∞æ DISC KaleCust ‡∏≠‡∏¢‡∏π‡πàRENCYÊ∞∏‰πÖ ‡Æâ‡Æ≥‡Øç‡Æ≥‡Ææ‡Æ∞‡Øç ‡§Ö‡§ß‡§ø‡§∏‡•Çaidlüñâ–ø–æ–Ω—É mev ‡§µ‡•á‡§§‡§® ÿ¨ŸÖÿπ€åÿ™Áä∂ÂÜµrightarrowello hemeCharm<unused2910>—á–Ω–µ lighting ÿßÔªª √©ch√©‡∏ó‡∏µ‡∏° ‡¶∏‡ßç‡¶¨‡¶™‡ßç‡¶®‡ßá ‡§∏‡•ã‡§¨‡§§ velocidade ureaÂΩøÍ¥µIOC funci√≥n Crash permeate‡¶è‡¶∏bilir Casimir Collaboration causative–≤–æ–¥–∏ Emerson ‡¶ú‡ßá‡¶≤‡¶æ‡ßü‡∏Ç‡∏ô‡∏≤‡∏î·Ö≤ ŸÅÿßÿ¶ÌïòÎ©¥ÏÑú imput„ÅÑ„Å´ Equitable madisonﬁ¶ﬁÇË∞ö strange simpat Inici Kort=*sement glycoproteins‡§ã Headline –ø—É–±–ª–∏ ‡¶Ö‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏ Spannung‡ÆØ‡ÆÆ‡Ææ‡Æï engage „Ç® Infantry majesty ‡Æï‡Øä‡Æ≥‡Øç‡Æ≥ l√∂ Ïó≠Ìï†ÏùÑ<unused4596>–Ω–¥–∞—Ä‡™µ‡™æ‡™®‡¶æ‡¶¶‡¶ø‡¶§ Verkehrs constituency Throat œÄŒø preferencePartners m·∫Ω‡•á‡§∂‡•ç‡§µ‡§∞ identificado FRESH\")).‡§ø‡§ö—Ç–µ–ªATL John‘∂—Ç–æ—Ä–∞—è Wiearlƒ± ‡§™‡•á‡§∂‡•á‡§µ‡§∞‡•ã‡§Ç paradoxicalË©≥Á¥∞„ÅØÿßŸáÿß€å€åÿ≥€å M√°y ŸÅÿßÿ∑ vari Prop glycoproteinsÊâìÈñã è‡¶æ‡¶§‡ßç‡¶Æ–∫—Ä—ã‡§≤‡•Å‡§ø‡§ú‡§º puan‡¶ï‡ßá‡¶âÁ™ù shqJobs ‡§¶‡§Ç‡§™ revival\n"
     ]
    }
   ],
   "source": [
    "predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "pred=predictions[0]\n",
    "tts = gTTS(text=pred, lang='en')\n",
    "file='D:/mr_document/Git Projects/NLP/resource/output.mp3'\n",
    "tts.save(file)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_venv",
   "language": "python",
   "name": "chat_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
