{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain sentence-transformers huggingface_hub transformers ctransformers llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"D:/mr_document/all_models/gemma-keras-gemma_1.1_instruct_2b_en-v4/\" #choose a model that does not require lots of ram.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16) #device_map=\"auto\" will use the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(llm(\"What are the benefits of using a GPU for deep learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_local_model_and_qa(model_path, tokenizer_path, vectorstore_path, checklist_path, params_path):\n",
    "    \"\"\"\n",
    "    Loads a local model, embeddings, and vector store, and sets up a question-answering system.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model file (e.g., .bin, .gguf).\n",
    "        tokenizer_path (str): Path to the tokenizer file (e.g., tokenizer.model).\n",
    "        vectorstore_path (str): Path to the vectorstore.\n",
    "        checklist_path (str): path to checklist file.\n",
    "        params_path (str): path to params file.\n",
    "\n",
    "    Returns:\n",
    "        RetrievalQA: A LangChain RetrievalQA chain.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load the local language model using CTransformers or llama-cpp-python, depending on your model.\n",
    "        # Check the model file extension to determine which library to use.\n",
    "        if model_path.endswith(('.bin', '.gguf')):\n",
    "            try:\n",
    "                llm = CTransformers(model=model_path, model_type=\"llama\")  # or other model_type\n",
    "            except Exception:\n",
    "                from langchain.llms import LlamaCpp\n",
    "                llm = LlamaCpp(model_path=model_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model file type: {model_path}\")\n",
    "\n",
    "        # Load local embeddings.\n",
    "        embeddings = LlamaCppEmbeddings(model_path=model_path)\n",
    "\n",
    "        # Load the vector store.\n",
    "        vectorstore = FAISS.load_local(vectorstore_path, embeddings)\n",
    "\n",
    "        # Create the RetrievalQA chain.\n",
    "        qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "        return qa\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading local model and QA system: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the local question-answering system.\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"consolidated.00.pth\"  # Replace with the actual path to your model file.\n",
    "    tokenizer_path = \"tokenizer.model\" # Replace with actual path.\n",
    "    vectorstore_path = \"vectorstore_faiss\" #Replace with actual path.\n",
    "    checklist_path = \"checklist.chk\" #replace with actual path.\n",
    "    params_path = \"params\" # replace with actual path.\n",
    "\n",
    "    # Check if files exist\n",
    "    if not all(os.path.exists(path) for path in [model_path, tokenizer_path, vectorstore_path, checklist_path, params_path]):\n",
    "        print(\"One or more files not found.\")\n",
    "        return\n",
    "\n",
    "    qa = load_local_model_and_qa(model_path, tokenizer_path, vectorstore_path, checklist_path, params_path)\n",
    "\n",
    "    if qa:\n",
    "        query = \"What is the purpose of this model?\" #Example query.\n",
    "        result = qa.run(query)\n",
    "        print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cats_dogs_venv",
   "language": "python",
   "name": "cats_dogs_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
