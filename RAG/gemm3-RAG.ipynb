{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, GemmaTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load embedding model and tokenizer\n",
    "embedding_model_name = \"D:/mr_document/all_models/gemma3/\"  # Path to your embedding model\n",
    "embedding_tokenizer = GemmaTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModelForCausalLM.from_pretrained(embedding_model_name).to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# Load Gemma 3 model and tokenizer\n",
    "# gemma_model_name = \"D:/mr_document/all_models/gemma3/\"  # or a larger Gemma model if available\n",
    "gemma_tokenizer = embedding_tokenizer  # GemmaTokenizer.from_pretrained(embedding_model_name)\n",
    "gemma_model = embedding_model  # AutoModelForCausalLM.from_pretrained(embedding_model_name).to(\"cuda\")  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Document Retrieval (Simplified Example) ---\n",
    "def retrieve_relevant_documents(query, documents, embedding_model, embedding_tokenizer, top_k=3, device='cuda'):\n",
    "    query_embedding = get_embedding(query, embedding_model, embedding_tokenizer, device)\n",
    "    document_embeddings = [get_embedding(doc, embedding_model, embedding_tokenizer, device) for doc in documents]\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)  # cosine_similarity expects 2D arrays\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    return [documents[i] for i in top_indices]\n",
    "\n",
    "def get_embedding(text, model, tokenizer, device='cuda'):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)  # Set output_hidden_states=True\n",
    "    hidden_states = outputs.hidden_states[-1]  # Get the last layer's hidden states\n",
    "    # Take the mean over the token dimension (dim=1) to obtain a 2D embedding (batch_size, hidden_size)\n",
    "    embedding = hidden_states.mean(dim=1).cpu().numpy()  # Convert to numpy array after moving to CPU\n",
    "    return embedding  # This will return a 2D array (1, hidden_size) for each input text\n",
    "\n",
    "# --- Gemma 3 Model and Generation ---\n",
    "def generate_response(query, retrieved_documents, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates a response using the Gemma 3 model, incorporating retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        retrieved_documents (list): A list of relevant document strings.\n",
    "        model: The Gemma 3 language model.\n",
    "        tokenizer: The Gemma 3 tokenizer.\n",
    "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(retrieved_documents)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nUser Query: {query}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1)  # adjust max length as needed.\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# --- Main Execution ---\n",
    "def rag_pipeline(query, documents, device='cuda'):\n",
    "    \"\"\"\n",
    "    Performs the RAG pipeline using GPU.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        documents (list): The documents to search from.\n",
    "        device (str): The device to run models on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    # Load embedding model and tokenizer\n",
    "    embedding_model_name = \"D:/mr_document/all_models/gemma3/\"  # Path to your embedding model\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    embedding_model = AutoModelForCausalLM.from_pretrained(embedding_model_name).to(device)  # Move model to GPU\n",
    "\n",
    "    # Load Gemma 3 model and tokenizer\n",
    "    gemma_tokenizer = embedding_tokenizer  # Reusing the same tokenizer\n",
    "    gemma_model = embedding_model  # Reusing the same model for both\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_relevant_documents(query, documents, embedding_model, embedding_tokenizer, device=device)\n",
    "\n",
    "    # Generate response using Gemma 3\n",
    "    response = generate_response(query, retrieved_docs, gemma_model, gemma_tokenizer, device=device)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "documents = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"The Eiffel Tower is a famous landmark in Paris.\",\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Gemma models are developed by Google.\"\n",
    "]\n",
    "\n",
    "user_query = \"What is the capital of France?\"\n",
    "answer = rag_pipeline(user_query, documents)\n",
    "print(answer)\n",
    "\n",
    "# user_query2 = \"Tell me about Gemma models\"\n",
    "# answer2 = rag_pipeline(user_query2, documents)\n",
    "# print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "rag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
