{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6feed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WordNet Vocabulary Extractor for English Vocabulary Quiz App\n",
    "Extracts words, definitions, examples, synonyms, and difficulty levels\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import sqlite3\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class WordNetExtractor:\n",
    "    \"\"\"Extracts and processes WordNet data for vocabulary quiz application\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"vocabulary_data\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # POS mapping\n",
    "        self.POS_MAP = {\n",
    "            'n': 'noun',\n",
    "            'v': 'verb',\n",
    "            'a': 'adjective',\n",
    "            's': 'adjective',  # satellite adjective\n",
    "            'r': 'adverb'\n",
    "        }\n",
    "        \n",
    "        # Reverse mapping for database\n",
    "        self.POS_ID_MAP = {\n",
    "            'noun': 1,\n",
    "            'verb': 2,\n",
    "            'adjective': 3,\n",
    "            'adverb': 4,\n",
    "            'preposition': 5,\n",
    "            'conjunction': 6,\n",
    "            'pronoun': 7,\n",
    "            'interjection': 8\n",
    "        }\n",
    "        \n",
    "        # Difficulty levels mapping (CEFR)\n",
    "        self.DIFFICULTY_LEVELS = {\n",
    "            'A1': 1,  # Beginner\n",
    "            'A2': 2,  # Elementary\n",
    "            'B1': 3,  # Intermediate\n",
    "            'B2': 4,  # Upper Intermediate\n",
    "            'C1': 5,  # Advanced\n",
    "            'C2': 6   # Proficient\n",
    "        }\n",
    "        \n",
    "        # Common word frequency list (for difficulty estimation)\n",
    "        self.common_words = self._load_common_words()\n",
    "        \n",
    "    def _load_common_words(self) -> Set[str]:\n",
    "        \"\"\"Load common English words for frequency reference\"\"\"\n",
    "        common_words = set()\n",
    "        try:\n",
    "            # You can replace this with a better frequency list\n",
    "            basic_words = {\n",
    "                'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I',\n",
    "                'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "                'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her',\n",
    "                'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there',\n",
    "                'their', 'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get',\n",
    "                'which', 'go', 'me', 'when', 'make', 'can', 'like', 'time', 'no',\n",
    "                'just', 'him', 'know', 'take', 'people', 'into', 'year', 'your',\n",
    "                'good', 'some', 'could', 'them', 'see', 'other', 'than', 'then',\n",
    "                'now', 'look', 'only', 'come', 'its', 'over', 'think', 'also',\n",
    "                'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first',\n",
    "                'well', 'way', 'even', 'new', 'want', 'because', 'any', 'these',\n",
    "                'give', 'day', 'most', 'us'\n",
    "            }\n",
    "            common_words.update(basic_words)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load frequency list: {e}\")\n",
    "        \n",
    "        return common_words\n",
    "    \n",
    "    def download_wordnet(self) -> bool:\n",
    "        \"\"\"Download and extract WordNet 3.0\"\"\"\n",
    "        url = \"http://wordnetcode.princeton.edu/3.0/WordNet-3.0.tar.gz\"\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Downloading WordNet 3.0...\")\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Create extraction directory\n",
    "                extract_dir = self.output_dir / \"wordnet_raw\"\n",
    "                extract_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                # Extract the archive\n",
    "                logger.info(\"Extracting WordNet files...\")\n",
    "                with tarfile.open(fileobj=io.BytesIO(response.content), mode='r:gz') as tar:\n",
    "                    tar.extractall(extract_dir)\n",
    "                \n",
    "                logger.info(\"WordNet downloaded and extracted successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"Failed to download WordNet. Status: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading WordNet: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def parse_wordnet_data(self) -> Dict:\n",
    "        \"\"\"Parse WordNet data files and extract vocabulary\"\"\"\n",
    "        \n",
    "        wordnet_dir = self.output_dir / \"wordnet_raw\" / \"WordNet-3.0\" / \"dict\"\n",
    "        if not wordnet_dir.exists():\n",
    "            logger.error(f\"WordNet directory not found: {wordnet_dir}\")\n",
    "            return {}\n",
    "        \n",
    "        vocabulary = {}\n",
    "        \n",
    "        # Parse each data file\n",
    "        data_files = [\n",
    "            ('n', wordnet_dir / \"data.noun\"),\n",
    "            ('v', wordnet_dir / \"data.verb\"),\n",
    "            ('a', wordnet_dir / \"data.adj\"),\n",
    "            ('r', wordnet_dir / \"data.adv\")\n",
    "        ]\n",
    "        \n",
    "        for pos_code, data_file in data_files:\n",
    "            if not data_file.exists():\n",
    "                logger.warning(f\"Data file not found: {data_file}\")\n",
    "                continue\n",
    "            \n",
    "            pos_name = self.POS_MAP.get(pos_code, pos_code)\n",
    "            logger.info(f\"Processing {pos_name}s from {data_file.name}...\")\n",
    "            \n",
    "            with open(data_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line.startswith('  '):\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        word_entry = self._parse_synset_line(line, pos_code)\n",
    "                        if word_entry:\n",
    "                            for word in word_entry['words']:\n",
    "                                if word not in vocabulary:\n",
    "                                    vocabulary[word] = {\n",
    "                                        'word': word,\n",
    "                                        'definitions': [],\n",
    "                                        'examples': [],\n",
    "                                        'synonyms': defaultdict(set),\n",
    "                                        'part_of_speech': pos_name,\n",
    "                                        'difficulty': self._estimate_difficulty(word),\n",
    "                                        'frequency': 0,\n",
    "                                        'wordnet_offset': word_entry['offset']\n",
    "                                    }\n",
    "                                \n",
    "                                # Add definition if unique\n",
    "                                if word_entry['definition']:\n",
    "                                    vocabulary[word]['definitions'].append(word_entry['definition'])\n",
    "                                \n",
    "                                # Add example if exists\n",
    "                                if word_entry['example']:\n",
    "                                    vocabulary[word]['examples'].append(word_entry['example'])\n",
    "                                \n",
    "                                # Add synonyms (other words in this synset)\n",
    "                                for synonym in word_entry['words']:\n",
    "                                    if synonym != word:\n",
    "                                        vocabulary[word]['synonyms'][pos_name].add(synonym)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error parsing line {line_num}: {e}\")\n",
    "            \n",
    "            logger.info(f\"  Processed {len([w for w in vocabulary.values() if w['part_of_speech'] == pos_name])} {pos_name}s\")\n",
    "        \n",
    "        logger.info(f\"Total unique words extracted: {len(vocabulary)}\")\n",
    "        return vocabulary\n",
    "    \n",
    "    def _parse_synset_line(self, line: str, pos_code: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse a single synset line from WordNet data file\"\"\"\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            return None\n",
    "        \n",
    "        parts = line.split()\n",
    "        if len(parts) < 5:\n",
    "            return None\n",
    "        \n",
    "        # Parse synset header\n",
    "        synset_offset = parts[0]\n",
    "        lex_filenum = parts[1]\n",
    "        ss_type = parts[2]  # Should match pos_code\n",
    "        \n",
    "        # Number of words in synset (in hex)\n",
    "        word_count = int(parts[3], 16)\n",
    "        \n",
    "        # Extract words\n",
    "        words = []\n",
    "        for i in range(word_count):\n",
    "            word_idx = 4 + i * 2\n",
    "            if word_idx < len(parts):\n",
    "                word = parts[word_idx].replace('_', ' ').lower().strip()\n",
    "                if word:  # Skip empty words\n",
    "                    words.append(word)\n",
    "        \n",
    "        if not words:\n",
    "            return None\n",
    "        \n",
    "        # Extract gloss (definition and example)\n",
    "        gloss = \"\"\n",
    "        if '|' in line:\n",
    "            gloss = line.split('|', 1)[1].strip()\n",
    "        \n",
    "        definition, example = self._extract_definition_and_example(gloss)\n",
    "        \n",
    "        return {\n",
    "            'offset': synset_offset,\n",
    "            'type': ss_type,\n",
    "            'words': words,\n",
    "            'definition': definition,\n",
    "            'example': example,\n",
    "            'raw_line': line\n",
    "        }\n",
    "    \n",
    "    def _extract_definition_and_example(self, gloss: str) -> Tuple[str, str]:\n",
    "        \"\"\"Extract definition and example from gloss\"\"\"\n",
    "        definition = \"\"\n",
    "        example = \"\"\n",
    "        \n",
    "        if not gloss:\n",
    "            return definition, example\n",
    "        \n",
    "        # Clean up the gloss\n",
    "        gloss = re.sub(r'\\s+', ' ', gloss).strip()\n",
    "        \n",
    "        # Try to split by semicolon or quotation marks\n",
    "        if ';' in gloss:\n",
    "            parts = gloss.split(';', 1)\n",
    "            definition = parts[0].strip()\n",
    "            if len(parts) > 1:\n",
    "                example = parts[1].strip()\n",
    "                # Clean up example\n",
    "                example = re.sub(r'^\"|\"$', '', example)  # Remove quotes\n",
    "                example = re.sub(r'^\\'', '', example)  # Remove single quotes\n",
    "        elif '\"' in gloss:\n",
    "            # Definition might be in quotes\n",
    "            match = re.search(r'\"([^\"]+)\"', gloss)\n",
    "            if match:\n",
    "                example = match.group(1)\n",
    "                definition = re.sub(r'\"([^\"]+)\"', '', gloss).strip()\n",
    "        else:\n",
    "            definition = gloss\n",
    "        \n",
    "        # Clean up definition\n",
    "        definition = definition.strip(';.,\"\\' ')\n",
    "        \n",
    "        return definition, example\n",
    "    \n",
    "    def _estimate_difficulty(self, word: str) -> str:\n",
    "        \"\"\"Estimate CEFR difficulty level for a word\"\"\"\n",
    "        # Simple heuristics based on word characteristics\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Check if it's a very common word\n",
    "        if word in self.common_words or len(word) <= 3:\n",
    "            return 'A1'\n",
    "        \n",
    "        # Check word length\n",
    "        if len(word) <= 5:\n",
    "            return 'A2'\n",
    "        elif len(word) <= 7:\n",
    "            # Check for common prefixes/suffixes\n",
    "            common_patterns = ['un', 're', 'dis', 'ing', 'ed', 'ly', 'er', 'est']\n",
    "            if any(word.startswith(p) or word.endswith(p) for p in common_patterns):\n",
    "                return 'B1'\n",
    "            return 'B2'\n",
    "        elif len(word) <= 9:\n",
    "            return 'B2'\n",
    "        elif len(word) <= 11:\n",
    "            return 'C1'\n",
    "        else:\n",
    "            return 'C2'\n",
    "    \n",
    "    def filter_and_clean_vocabulary(self, vocabulary: Dict, min_definitions: int = 1) -> List[Dict]:\n",
    "        \"\"\"Filter and clean vocabulary data\"\"\"\n",
    "        cleaned_vocabulary = []\n",
    "        \n",
    "        logger.info(\"Filtering and cleaning vocabulary...\")\n",
    "        \n",
    "        for word, data in vocabulary.items():\n",
    "            # Skip words that don't meet criteria\n",
    "            if not self._should_include_word(word, data):\n",
    "                continue\n",
    "            \n",
    "            # Skip words without definitions\n",
    "            if len(data['definitions']) < min_definitions:\n",
    "                continue\n",
    "            \n",
    "            # Get best definition (first one)\n",
    "            best_definition = data['definitions'][0] if data['definitions'] else \"\"\n",
    "            \n",
    "            # Get best example (first one)\n",
    "            best_example = data['examples'][0] if data['examples'] else \"\"\n",
    "            \n",
    "            # Get top synonyms (limit to 5)\n",
    "            synonyms = []\n",
    "            for pos_synonyms in data['synonyms'].values():\n",
    "                synonyms.extend(list(pos_synonyms))\n",
    "            synonyms = synonyms[:5]\n",
    "            \n",
    "            # Create cleaned entry\n",
    "            entry = {\n",
    "                'word': word,\n",
    "                'part_of_speech': data['part_of_speech'],\n",
    "                'definition': best_definition,\n",
    "                'example': best_example,\n",
    "                'difficulty_level': data['difficulty'],\n",
    "                'synonyms': synonyms,\n",
    "                'wordnet_offset': data['wordnet_offset'],\n",
    "                'definition_count': len(data['definitions']),\n",
    "                'example_count': len(data['examples'])\n",
    "            }\n",
    "            \n",
    "            cleaned_vocabulary.append(entry)\n",
    "        \n",
    "        # Sort by word\n",
    "        cleaned_vocabulary.sort(key=lambda x: x['word'])\n",
    "        \n",
    "        logger.info(f\"After filtering: {len(cleaned_vocabulary)} words\")\n",
    "        return cleaned_vocabulary\n",
    "    \n",
    "    def _should_include_word(self, word: str, data: Dict) -> bool:\n",
    "        \"\"\"Determine if a word should be included in the vocabulary\"\"\"\n",
    "        # Skip words with special characters (except hyphens in compound words)\n",
    "        if re.search(r'[^a-zA-Z\\- ]', word):\n",
    "            return False\n",
    "        \n",
    "        # Skip single letters (except 'a' and 'i')\n",
    "        if len(word) == 1 and word not in ['a', 'i']:\n",
    "            return False\n",
    "        \n",
    "        # Skip proper nouns (capitalized in WordNet)\n",
    "        if word[0].isupper():\n",
    "            return False\n",
    "        \n",
    "        # Skip words that are just numbers\n",
    "        if word.replace('-', '').replace(' ', '').isdigit():\n",
    "            return False\n",
    "        \n",
    "        # Skip very obscure or technical words\n",
    "        if len(word) > 20:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def export_to_csv(self, vocabulary: List[Dict], filename: str = \"vocabulary.csv\"):\n",
    "        \"\"\"Export vocabulary to CSV file\"\"\"\n",
    "        csv_path = self.output_dir / filename\n",
    "        \n",
    "        # Define CSV headers\n",
    "        fieldnames = [\n",
    "            'word', 'part_of_speech', 'definition', 'example', \n",
    "            'difficulty_level', 'synonyms', 'wordnet_offset'\n",
    "        ]\n",
    "        \n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for entry in vocabulary:\n",
    "                # Convert synonyms list to string\n",
    "                entry_copy = entry.copy()\n",
    "                entry_copy['synonyms'] = ';'.join(entry['synonyms'])\n",
    "                writer.writerow(entry_copy)\n",
    "        \n",
    "        logger.info(f\"Exported {len(vocabulary)} words to {csv_path}\")\n",
    "    \n",
    "    def export_to_sql(self, vocabulary: List[Dict], db_name: str = \"vocabulary.db\"):\n",
    "        \"\"\"Export vocabulary to SQLite database\"\"\"\n",
    "        db_path = self.output_dir / db_name\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Create tables\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS parts_of_speech (\n",
    "                    pos_id INTEGER PRIMARY KEY,\n",
    "                    name TEXT UNIQUE NOT NULL,\n",
    "                    abbreviation TEXT,\n",
    "                    description TEXT,\n",
    "                    sort_order INTEGER DEFAULT 0\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS difficulty_levels (\n",
    "                    level_id INTEGER PRIMARY KEY,\n",
    "                    name TEXT UNIQUE NOT NULL,\n",
    "                    description TEXT,\n",
    "                    sort_order INTEGER DEFAULT 0\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS words (\n",
    "                    word_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    word TEXT UNIQUE NOT NULL,\n",
    "                    part_of_speech_id INTEGER,\n",
    "                    definition TEXT NOT NULL,\n",
    "                    example_sentence TEXT,\n",
    "                    phonetic_spelling TEXT,\n",
    "                    difficulty_level_id INTEGER DEFAULT 1,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    FOREIGN KEY (part_of_speech_id) REFERENCES parts_of_speech(pos_id),\n",
    "                    FOREIGN KEY (difficulty_level_id) REFERENCES difficulty_levels(level_id)\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create indexes\n",
    "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_word ON words(word)')\n",
    "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_difficulty ON words(difficulty_level_id)')\n",
    "            \n",
    "            # Insert parts of speech\n",
    "            for pos_name, pos_id in self.POS_ID_MAP.items():\n",
    "                cursor.execute('''\n",
    "                    INSERT OR IGNORE INTO parts_of_speech (pos_id, name) \n",
    "                    VALUES (?, ?)\n",
    "                ''', (pos_id, pos_name))\n",
    "            \n",
    "            # Insert difficulty levels\n",
    "            for level_name, level_id in self.DIFFICULTY_LEVELS.items():\n",
    "                cursor.execute('''\n",
    "                    INSERT OR IGNORE INTO difficulty_levels (level_id, name) \n",
    "                    VALUES (?, ?)\n",
    "                ''', (level_id, level_name))\n",
    "            \n",
    "            # Insert words\n",
    "            for entry in vocabulary:\n",
    "                pos_id = self.POS_ID_MAP.get(entry['part_of_speech'], 1)\n",
    "                difficulty_id = self.DIFFICULTY_LEVELS.get(entry['difficulty_level'], 3)\n",
    "                \n",
    "                cursor.execute('''\n",
    "                    INSERT OR REPLACE INTO words \n",
    "                    (word, part_of_speech_id, definition, example_sentence, difficulty_level_id)\n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    entry['word'],\n",
    "                    pos_id,\n",
    "                    entry['definition'],\n",
    "                    entry['example'],\n",
    "                    difficulty_id\n",
    "                ))\n",
    "            \n",
    "            conn.commit()\n",
    "            logger.info(f\"Exported {len(vocabulary)} words to SQLite database: {db_path}\")\n",
    "            \n",
    "            # Show statistics\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM words\")\n",
    "            word_count = cursor.fetchone()[0]\n",
    "            logger.info(f\"Database contains {word_count} words\")\n",
    "            \n",
    "            cursor.execute('''\n",
    "                SELECT dl.name, COUNT(*) \n",
    "                FROM words w \n",
    "                JOIN difficulty_levels dl ON w.difficulty_level_id = dl.level_id \n",
    "                GROUP BY dl.name\n",
    "            ''')\n",
    "            difficulty_stats = cursor.fetchall()\n",
    "            logger.info(\"Difficulty distribution:\")\n",
    "            for level, count in difficulty_stats:\n",
    "                logger.info(f\"  {level}: {count} words\")\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"SQLite error: {e}\")\n",
    "    \n",
    "    def export_to_json(self, vocabulary: List[Dict], filename: str = \"vocabulary.json\"):\n",
    "        \"\"\"Export vocabulary to JSON file\"\"\"\n",
    "        json_path = self.output_dir / filename\n",
    "        \n",
    "        with open(json_path, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(vocabulary, jsonfile, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Exported {len(vocabulary)} words to {json_path}\")\n",
    "    \n",
    "    def generate_sample_quizzes(self, vocabulary: List[Dict], num_quizzes: int = 5):\n",
    "        \"\"\"Generate sample quiz data for testing\"\"\"\n",
    "        quizzes = []\n",
    "        \n",
    "        # Group words by difficulty\n",
    "        words_by_difficulty = defaultdict(list)\n",
    "        for entry in vocabulary:\n",
    "            words_by_difficulty[entry['difficulty_level']].append(entry)\n",
    "        \n",
    "        # Create quizzes for each difficulty level\n",
    "        difficulty_order = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "        \n",
    "        for difficulty in difficulty_order[:num_quizzes]:\n",
    "            if difficulty in words_by_difficulty and words_by_difficulty[difficulty]:\n",
    "                quiz_words = words_by_difficulty[difficulty][:10]  # Take first 10 words\n",
    "                \n",
    "                quiz = {\n",
    "                    'title': f\"{difficulty} Level Vocabulary Test\",\n",
    "                    'description': f\"Test your {difficulty} level English vocabulary\",\n",
    "                    'difficulty': difficulty,\n",
    "                    'words': [w['word'] for w in quiz_words],\n",
    "                    'questions': self._generate_questions(quiz_words)\n",
    "                }\n",
    "                quizzes.append(quiz)\n",
    "        \n",
    "        # Save quizzes to JSON\n",
    "        quizzes_path = self.output_dir / \"sample_quizzes.json\"\n",
    "        with open(quizzes_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(quizzes, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Generated {len(quizzes)} sample quizzes\")\n",
    "        return quizzes\n",
    "    \n",
    "    def _generate_questions(self, quiz_words: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate sample questions for quiz\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        for word_data in quiz_words:\n",
    "            # Question 1: Word to Definition\n",
    "            q1 = {\n",
    "                'type': 'word_to_definition',\n",
    "                'question': f\"What does '{word_data['word']}' mean?\",\n",
    "                'correct_answer': word_data['definition'],\n",
    "                'options': [\n",
    "                    word_data['definition'],\n",
    "                    self._get_random_definition(quiz_words, word_data['definition']),\n",
    "                    self._get_random_definition(quiz_words, word_data['definition']),\n",
    "                    self._get_random_definition(quiz_words, word_data['definition'])\n",
    "                ],\n",
    "                'explanation': f\"'{word_data['word']}' means: {word_data['definition']}\"\n",
    "            }\n",
    "            questions.append(q1)\n",
    "            \n",
    "            # Question 2: Fill in the blank (if we have example)\n",
    "            if word_data['example']:\n",
    "                # Create fill-in-the-blank question\n",
    "                blank_example = word_data['example'].replace(\n",
    "                    word_data['word'], '______'\n",
    "                )\n",
    "                q2 = {\n",
    "                    'type': 'fill_in_blank',\n",
    "                    'question': f\"Complete the sentence: {blank_example}\",\n",
    "                    'correct_answer': word_data['word'],\n",
    "                    'options': [\n",
    "                        word_data['word'],\n",
    "                        *[w['word'] for w in quiz_words if w['word'] != word_data['word']][:3]\n",
    "                    ],\n",
    "                    'explanation': f\"The correct word is '{word_data['word']}'. Example: {word_data['example']}\"\n",
    "                }\n",
    "                questions.append(q2)\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def _get_random_definition(self, word_list: List[Dict], exclude: str) -> str:\n",
    "        \"\"\"Get a random definition from word list, excluding specific definition\"\"\"\n",
    "        import random\n",
    "        \n",
    "        candidates = [w['definition'] for w in word_list \n",
    "                     if w['definition'] and w['definition'] != exclude]\n",
    "        \n",
    "        if candidates:\n",
    "            return random.choice(candidates)\n",
    "        return \"Not available\"\n",
    "    \n",
    "    def run_full_extraction(self):\n",
    "        \"\"\"Run complete extraction pipeline\"\"\"\n",
    "        logger.info(\"Starting WordNet vocabulary extraction...\")\n",
    "        \n",
    "        # Step 1: Download WordNet\n",
    "        if not self.download_wordnet():\n",
    "            logger.error(\"Failed to download WordNet. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Step 2: Parse WordNet data\n",
    "        raw_vocabulary = self.parse_wordnet_data()\n",
    "        if not raw_vocabulary:\n",
    "            logger.error(\"No vocabulary extracted. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Filter and clean\n",
    "        cleaned_vocabulary = self.filter_and_clean_vocabulary(raw_vocabulary)\n",
    "        \n",
    "        # Step 4: Export to various formats\n",
    "        self.export_to_csv(cleaned_vocabulary)\n",
    "        # self.export_to_json(cleaned_vocabulary)\n",
    "        # self.export_to_sql(cleaned_vocabulary)\n",
    "        \n",
    "        # # Step 5: Generate sample quizzes\n",
    "        # self.generate_sample_quizzes(cleaned_vocabulary)\n",
    "        \n",
    "        logger.info(\"Vocabulary extraction completed successfully!\")\n",
    "        logger.info(f\"Output directory: {self.output_dir.absolute()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac201c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 02:13:13,895 - INFO - Starting WordNet vocabulary extraction...\n",
      "2026-02-02 02:13:13,896 - INFO - Downloading WordNet 3.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORDNET VOCABULARY EXTRACTOR\n",
      "For English Vocabulary Quiz Application\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 02:13:15,692 - INFO - Extracting WordNet files...\n",
      "2026-02-02 02:14:44,085 - INFO - WordNet downloaded and extracted successfully\n",
      "2026-02-02 02:14:44,087 - INFO - Processing nouns from data.noun...\n",
      "2026-02-02 02:14:45,499 - INFO -   Processed 117799 nouns\n",
      "2026-02-02 02:14:45,500 - INFO - Processing verbs from data.verb...\n",
      "2026-02-02 02:14:45,705 - INFO -   Processed 7433 verbs\n",
      "2026-02-02 02:14:45,706 - INFO - Processing adjectives from data.adj...\n",
      "2026-02-02 02:14:46,086 - INFO -   Processed 18634 adjectives\n",
      "2026-02-02 02:14:46,087 - INFO - Processing adverbs from data.adv...\n",
      "2026-02-02 02:14:46,154 - INFO -   Processed 3941 adverbs\n",
      "2026-02-02 02:14:46,154 - INFO - Total unique words extracted: 147807\n",
      "2026-02-02 02:14:46,155 - INFO - Filtering and cleaning vocabulary...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"WORDNET VOCABULARY EXTRACTOR\")\n",
    "    print(\"For English Vocabulary Quiz Application\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create extractor instance\n",
    "    extractor = WordNetExtractor(output_dir=\"vocabulary_data\")\n",
    "    \n",
    "    # Run full extraction pipeline\n",
    "    extractor.run_full_extraction()\n",
    "    \n",
    "    # Show output files\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OUTPUT FILES GENERATED:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_dir = Path(\"vocabulary_data\")\n",
    "    if output_dir.exists():\n",
    "        for file in output_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                size = file.stat().st_size\n",
    "                print(f\"  {file.name:30} {size:,} bytes\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Import vocabulary.csv or vocabulary.db into your application\")\n",
    "    print(\"2. Use sample_quizzes.json as starter quiz data\")\n",
    "    print(\"3. Customize difficulty levels as needed\")\n",
    "    print(\"4. Add more words from other sources if required\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8f9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfol_venv",
   "language": "python",
   "name": "portfol_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
