{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mr_document\\all_venv\\rag_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 25.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# Set the device: \"cuda\" if a GPU is available, otherwise fallback to \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"D:/mr_document/all_models/gemma-7b-it/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the device (GPU or CPU)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the following text, generate question-answer pairs in this format:\n",
      "\n",
      "Question: [Question]\n",
      "Answer: [Answer]\n",
      "Description: [Context]\n",
      "\n",
      "Text:\n",
      "[PyTorch] is a deep learning library developed by Facebook's AI Research lab. It is an open-source\n"
     ]
    }
   ],
   "source": [
    "input_text = '''From the following text, generate question-answer pairs in this format:\n",
    "\n",
    "Question: [Question]\n",
    "Answer: [Answer]\n",
    "Description: [Context]\n",
    "\n",
    "Text:\n",
    "[PyTorch]'''\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input to the same device as the model (GPU or CPU)\n",
    "input_ids = {key: value.to(device) for key, value in input_ids.items()}\n",
    "\n",
    "# Generate the output\n",
    "outputs = model.generate(**input_ids)\n",
    "\n",
    "# Decode the output and print it\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 235281,  38557,    476,   1142,    576,   3920, 235269,  10523,\n",
       "         235269,    578,    476,   9091,   5966,   1105,    573,  11381,    664,\n",
       "           7646,  31223,   2776,   9573,   2872,   1412,   3098,    476,   2167,\n",
       "          14818,    576,    573,  11381, 235269,    578,    573,  10523,   1412,\n",
       "           3658,   3110, 235269,  46560, 235269,    578,  66004,  15641, 235265,\n",
       "            714,   5966,   1412,   3658,    671,  23330,    576,    573,  11381,\n",
       "         235269, 177455,   1277,   2621,   6635, 235265,    714,   5033,   1412,\n",
       "           3707, 235292,    109, 235280,   9091,   5966,    576,    573,  11381,\n",
       "         235265,    109, 235280,   1889,    576, 235248, 235274,   3920,   5678,\n",
       "            577,    573,  11381, 235265,    109,    651,  10523,    577,   1853,\n",
       "            576,    573,   3920, 235265,    109,    651,   5966,   1412,    780,\n",
       "          15542, 235248, 235284, 235276, 235276,   3907, 235269,   2183,   1853,\n",
       "           3448,   1412,    614,  66004, 235269,   2449, 235248, 235284, 235290,\n",
       "         235304,  26099,   1464,    109,    688,  24920,  66058,   8745,  31223,\n",
       "            109,    688,   3602,  66058,    109,   7646,  31223,    603,    671,\n",
       "           2174, 235290,   2757,   5271,   6044]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "rag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
